%Experiment
\section{Experiment}\label{sec:experiment}
This section explains the experimental methodology, along with a description of the computational environment and datasets used. The research results for each method are then described, along with commentary on the results obtained.

\subsection{Dataset and implementation description}

The experiments were conducted using modified Solomon benchmark instances, which are widely recognized in the vehicle routing problem research community. These instances are divided into six categories based on their characteristics: C1 (clustered customers with narrow time windows), C2 (clustered customers with wide time windows), R1 (randomly distributed customers with narrow time windows), R2 (randomly distributed customers with wide time windows), RC1 (mixed clustered and random distribution with narrow time windows), and RC2 (mixed distribution with wide time windows). Each instance contains 100 customer locations with defined coordinates, time windows, and service requirements.

The problem was adapted to the HCVRP-TW formulation by introducing heterogeneous crew types (junior and senior), dual shift system (morning and afternoon), overtime allowances, soft time windows with penalties, and vehicle capacity constraints for consumables transport. All experiments were executed on a computational platform with sufficient resources to ensure consistent performance measurements across all algorithmic approaches.

\subsection{Optimization factors and quality measure}

The quality of solutions is measured by the objective function $F(\pi)$, which represents the total operational cost. This function is composed of four components: driving cost $F_1(\pi)$, overtime cost $F_2(\pi)$, time window penalty cost $F_3(\pi)$, and crew deployment cost $F_4(\pi)$. The driving cost accounts for the distance traveled by all vehicles. The overtime cost penalizes working hours exceeding the standard shift duration, with rates dependent on crew qualification levels. Time window penalties are applied for both early arrivals and late departures from customer locations. The crew cost represents the fixed cost of deploying a crew for the day.

For comparative analysis, three algorithmic approaches were implemented: a simple greedy heuristic (1-nearest neighbor), an adapted Tabu Search algorithm, and an Artificial Bee Colony (ABC) algorithm. The greedy heuristic serves as a baseline, constructing solutions by iteratively selecting the nearest unvisited location. The Tabu Search implementation uses a tabu list to avoid revisiting recent solutions, with various neighborhood operators to explore the solution space. The ABC algorithm mimics the foraging behavior of honey bees, with employed and onlooker bees exploring solutions and scout bees performing diversification.

\subsection{Experiment results}

\subsubsection{Algorithm comparison on benchmark instances}

Table \ref{tab:comparison} presents a comprehensive comparison of the three implemented algorithms across all 56 Solomon benchmark instances. For each instance, the table shows the total cost achieved by each algorithm (Greedy, Tabu Search, and Bee Colony) as well as the relative improvement ratios between algorithms. The improvement ratio is calculated as $(Algorithm_A - Algorithm_B) / Algorithm_A$, indicating the percentage improvement of Algorithm B over Algorithm A.

Across the C-type instances (clustered customers), the Greedy algorithm produced costs ranging from 12,625 to 45,135, while Tabu Search achieved costs between 4,847 and 15,363, and the Bee algorithm obtained results between 5,907 and 20,652. For R-type instances (randomly distributed customers), Greedy costs ranged from 6,621 to 13,334, Tabu Search from 2,466 to 8,325, and Bee from 3,198 to 9,424. The RC-type instances (mixed distribution) showed Greedy costs from 8,766 to 13,946, Tabu costs from 3,056 to 8,046, and Bee costs from 4,444 to 9,798.

The improvement ratios reveal that Tabu Search consistently outperformed the Greedy baseline, with improvements ranging from 20.04\% to 83.84\% across different instances. The Bee algorithm also showed substantial improvements over Greedy, ranging from 3.81\% to 79.22\%. When comparing Tabu Search to Bee algorithm, the results varied by instance, with improvements ranging from -39.96\% to 60.00\%, indicating that the relative performance depends on instance characteristics.

\input{../comparison}

\subsubsection{Aggregated results by instance type}

To better understand the performance patterns across different problem characteristics, the results were aggregated by instance type. The mean costs for each instance category are presented in Figure \ref{fig:mean_comparision}. For C1 instances (clustered with narrow time windows), the mean costs were 17,201.11 for Greedy, 10,183.33 for Tabu Search, and 14,173.00 for Bee algorithm. The C2 instances (clustered with wide time windows) showed mean costs of 31,076.88 (Greedy), 6,357.25 (Tabu), and 9,434.75 (Bee).

The R1 instances (random with narrow time windows) yielded mean costs of 8,564.25 (Greedy), 6,405.08 (Tabu), and 7,564.09 (Bee). For R2 instances (random with wide time windows), the means were 9,612.27 (Greedy), 3,008.82 (Tabu), and 4,364.36 (Bee). The RC1 instances (mixed with narrow time windows) produced means of 9,763.63 (Greedy), 7,058.25 (Tabu), and 8,304.13 (Bee), while RC2 instances (mixed with wide time windows) showed means of 11,443.88 (Greedy), 3,694.75 (Tabu), and 5,644.88 (Bee).

\input{../mean_reults_comparision}

\subsubsection{Cost component analysis}

The total cost function consists of multiple components, each contributing differently to the overall solution quality. Table \ref{tab:comp_TotalPenalty} presents the time window penalty costs across instance types. For C1 instances, the mean penalties were 9,566.78 (Greedy), 3,439.00 (Tabu), and 4,502.67 (Bee), representing reductions of 64.05\% and 52.93\% respectively over the baseline. The C2 instances showed even more dramatic improvements, with penalties of 24,480.00 (Greedy), 1,438.88 (Tabu), and 3,456.62 (Bee), corresponding to 94.12\% and 85.88\% reductions.

The random distribution instances (R1 and R2) exhibited similar patterns. R1 penalties were 1,666.83 (Greedy), 950.83 (Tabu), and 1,349.00 (Bee), with reductions of 42.96\% and 19.07\%. R2 instances had penalties of 4,242.73 (Greedy), 151.82 (Tabu), and 332.64 (Bee), showing reductions of 96.42\% and 92.16\%. The mixed distribution instances (RC1 and RC2) demonstrated penalties of 1,850.00, 834.50, 1,241.50 for RC1 and 4,881.38, 245.12, 612.88 for RC2, with corresponding reduction percentages.

\input{../comparison_TotalPenalty}

Table \ref{tab:comp_CrewCost} shows the crew deployment costs across algorithms and instance types. For C1 instances, the costs were relatively stable: 3,777.78 (Greedy), 3,788.89 (Tabu), and 3,644.44 (Bee), showing minimal variation. However, C2 instances demonstrated more significant differences: 3,287.50 (Greedy), 1,775.00 (Tabu), and 2,300.00 (Bee), representing reductions of 46.01\% and 30.04\% respectively.

The R1 instances showed crew costs of 3,041.67 (Greedy), 2,533.33 (Tabu), and 2,636.36 (Bee), with reductions of 16.71\% and 13.33\%. R2 instances had costs of 2,818.18 (Greedy), 1,163.64 (Tabu), and 1,518.18 (Bee), showing reductions of 58.71\% and 46.13\%. The RC-type instances followed similar trends, with RC1 showing costs of 3,337.50, 2,762.50, 2,975.00 and RC2 showing 3,212.50, 1,400.00, 1,900.00.

\input{../comparison_CrewCost}

\subsubsection{Tabu Search parameter sensitivity}

The Tabu Search algorithm's performance was evaluated with different tabu list sizes to determine the optimal configuration. Figure \ref{fig:mean_goals_per_tabu_size_all_instances} shows the mean objective function values for tabu sizes of 20, 40, and 80 across all instances. The results indicate mean costs of 11,864.21 for tabu size 20, 11,832.96 for size 40, and 11,514.04 for size 80. The data shows a general trend of performance improvement with larger tabu list sizes, with the most significant reduction occurring when increasing from size 20 to 80, representing a 2.95\% improvement.

\input{../mean_goals_per_tabu_size_all_instances}

Additionally, different neighborhood operators were tested for the Tabu Search implementation. Figure \ref{fig:mean_goals_per_move_type_all_instances} presents the mean objective values for four operator types: SwapMove (10,430.00), InsertMove (9,243.78), ReverseMove (13,557.11), and TwoOptMove (13,717.39). The InsertMove operator demonstrated the best performance, achieving 11.38\% lower costs than SwapMove and 31.95\% lower than TwoOptMove. The SwapMove operator ranked second, while ReverseMove and TwoOptMove showed less favorable results for this problem variant.

\input{../mean_goals_per_move_type_all_instances}

\subsubsection{Artificial Bee Colony parameter sensitivity}

The Bee algorithm's performance was analyzed with respect to several key parameters. Figure \ref{fig:mean_goals_per_bee_number_all_instances} shows the impact of colony size on solution quality, testing configurations with 25, 50, and 75 bees. The mean objective values were 20,727.27 for 25 bees, 19,040.54 for 50 bees, and 18,678.06 for 75 bees. The results demonstrate a clear improvement trend with increased colony size, with the 75-bee configuration achieving 9.89\% better performance than the 25-bee configuration.

\input{../mean_goals_per_bee_number_all_instances}

The limit parameter, which controls when scout bees replace abandoned food sources, was tested with values of 20 and 40. Figure \ref{fig:mean_goals_per_limit_all_instances} presents mean costs of 20,753.13 for limit 20 and 18,210.79 for limit 40. The higher limit value resulted in 12.25\% better performance, suggesting that allowing more exploration attempts before abandoning a solution region benefits the search process.

\input{../mean_goals_per_limit_all_instances}

Different operator types for neighborhood exploration in the Bee algorithm were also evaluated. Figure \ref{fig:mean_goals_per_bee_operator_all_instances} shows results for SwapMove (14,829.42), InsertMove (13,836.47), ReverseMove (21,299.36), and TwoOptMove (27,962.58). Consistent with the Tabu Search findings, InsertMove proved most effective, while TwoOptMove performed worst, showing an 88.55\% cost increase compared to InsertMove.

\input{../mean_goals_per_bee_operator_all_instances}

\subsection{Findings and interpretation}

The experimental results reveal several important patterns regarding the performance of different algorithmic approaches for the HCVRP-TW problem. First, both metaheuristic approaches (Tabu Search and Artificial Bee Colony) substantially outperform the simple greedy baseline across all instance types, demonstrating the value of sophisticated search strategies for this complex optimization problem. The magnitude of improvement varies significantly with instance characteristics, with the most dramatic gains observed in C2 and R2 instances (those with wide time windows).

The superior performance on wide time window instances can be attributed to the larger solution space these problems afford. Wide time windows provide more flexibility in scheduling visits, allowing metaheuristics to explore diverse routing configurations and discover solutions that effectively balance multiple cost components. Conversely, narrow time window instances (C1, R1, RC1) impose stricter temporal constraints that limit solution space exploration, resulting in smaller but still substantial improvements over the greedy approach.

Comparing the two metaheuristics, Tabu Search generally achieves lower total costs than the Bee algorithm across most instances. However, the Bee algorithm demonstrates competitive performance and, in some cases, matches or exceeds Tabu Search results, particularly on certain R-type and RC-type instances. This suggests that both algorithms have merit and their relative effectiveness depends on specific problem characteristics.

The cost component analysis provides insight into how the algorithms achieve their improvements. The substantial reductions in time window penalties indicate that both metaheuristics excel at finding routes that respect customer availability constraints. This is particularly evident in the C2 and R2 instance categories, where penalty reductions exceed 90\% compared to the greedy baseline. The crew cost reductions demonstrate that metaheuristics successfully consolidate customer visits onto fewer routes, thereby minimizing the number of deployed crews and associated fixed costs.

The parameter sensitivity analyses reveal important tuning considerations. For Tabu Search, larger tabu list sizes (80) provide better performance than smaller sizes (20), suggesting that maintaining a more extensive memory of recent moves helps avoid local optima. The InsertMove operator proves most effective for neighborhood exploration in this problem domain, likely because insertion operations provide a good balance between solution modification intensity and computational efficiency.

For the Artificial Bee Colony algorithm, increasing colony size from 25 to 75 bees improves performance, though with diminishing returns. The limit parameter analysis indicates that allowing more exploration attempts before abandoning solution regions (limit=40 vs limit=20) benefits solution quality. The consistency in operator performance between Tabu Search and ABC - with InsertMove outperforming other operators in both cases - suggests that this operator type is fundamentally well-suited to the neighborhood structure of HCVRP-TW problems.

\subsection{Conclusions}

The computational experiments demonstrate that the HCVRP-TW problem, incorporating heterogeneous crews, dual shifts, overtime considerations, and soft time windows, can be effectively addressed using metaheuristic optimization approaches. The implemented Tabu Search and Artificial Bee Colony algorithms both produce high-quality solutions that substantially improve upon baseline greedy construction heuristics.

The results address several research objectives established in the introduction. Regarding PG3 (efficient solving with commercial solvers), while MILP solver performance was not the primary focus of this experimental section, the substantial problem complexity evidenced by the wide range of solutions and cost components suggests that metaheuristics provide a practical alternative for instances where exact methods may struggle. For PG4 (comparing local search and swarm algorithms), the experiments show that Tabu Search generally outperforms the Bee algorithm but that both approaches deliver significant value, with the choice potentially depending on computational budget and solution quality requirements.

Concerning PG5 (impact of time windows and crew qualifications), the aggregated results by instance type clearly demonstrate that time window width significantly affects solution difficulty and the magnitude of achievable improvements. Instances with wide time windows (C2, R2, RC2) show the largest improvements from metaheuristic optimization, while narrow window instances exhibit more modest but still meaningful gains. The crew cost analysis reveals that algorithm sophistication enables better crew utilization and route consolidation.

Future research directions suggested by these results include: (1) hybrid approaches combining Tabu Search and Artificial Bee Colony to leverage their complementary strengths, (2) adaptive parameter tuning mechanisms that adjust algorithm behavior based on instance characteristics, (3) multi-objective optimization variants that explicitly trade off different cost components, and (4) investigation of parallel computing strategies to further improve solution quality within practical time limits.

The experimental evidence supports the conclusion that metaheuristic approaches provide an effective and practical solution methodology for real-world building maintenance routing problems with complex constraints. The insights gained regarding parameter sensitivity and algorithm behavior provide guidance for practitioners implementing these methods in operational settings.
